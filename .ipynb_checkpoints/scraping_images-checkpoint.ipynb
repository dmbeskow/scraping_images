{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Images\n",
    "\n",
    "This code is a collection of code that I used for scraping images from a variety of locations (Google images, Flickr, Tumblr, Twitter, and Instagram).\n",
    "\n",
    "I used these platforms to collect images for training a Convolutional Neural Network model.  \n",
    "\n",
    "In my case I was building a CNN to detect internet memes, and used the platforms above to get ~20K examples of memes.\n",
    "\n",
    "So let's dive and show how to scrape images on each of these platforms:\n",
    "\n",
    "### Google\n",
    "\n",
    "I used the code below in conjunction with Selenium (Chrome Driver) to scrape Google Images.  I did this with manual scroll down since I only had a handful of queries, but it would probably be trivial to automate the scrolling.\n",
    "\n",
    "First, load the packages and function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import argparse\n",
    "import requests\n",
    "import io\n",
    "import hashlib\n",
    "import itertools\n",
    "import base64\n",
    "from PIL import Image\n",
    "from multiprocessing import Pool\n",
    "from selenium import webdriver\n",
    "\n",
    "#%%\n",
    "def persist_image(dir_image_src):\n",
    "    label_directory = dir_image_src[0]\n",
    "    image_src = dir_image_src[1]\n",
    "\n",
    "    size = (200, 200)\n",
    "    try:\n",
    "        image_content = requests.get(image_src).content\n",
    "    except requests.exceptions.InvalidSchema:\n",
    "        # image is probably base64 encoded\n",
    "        image_data = re.sub('^data:image/.+;base64,', '', image_src)\n",
    "        image_content = base64.b64decode(image_data)\n",
    "    except Exception as e:\n",
    "        print(\"could not read\", e, image_src)\n",
    "        return False\n",
    "\n",
    "    image_file = io.BytesIO(image_content)\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    with open('graphs/' + hashlib.sha1(image_content).hexdigest() + \".jpg\", 'wb')  as h:\n",
    "        image.save(h, \"JPEG\", quality=85)\n",
    "    resized = image.resize(size)\n",
    "    with open(label_directory + hashlib.sha1(image_content).hexdigest() + \".jpg\", 'wb')  as f:\n",
    "        resized.save(f, \"JPEG\", quality=85)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the query and then scroll down to load all of the pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# The method below is a bit manual (meaning it requires the user to scroll down), but was adequate\n",
    "# for my needs since I only had a few queries.  It wouldn't be hard to automate the scrolling if you\n",
    "# have hundreds of queries\n",
    "query = 'memes'\n",
    "\n",
    "image_urls = set()\n",
    "\n",
    "search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
    "\n",
    "browser = webdriver.Chrome('C:/Users/dmbes/Downloads/chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "browser.get(search_url.format(q=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all of the URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now scroll down to bottom of file\n",
    "\n",
    "images = browser.find_elements_by_css_selector(\"img.rg_ic\")\n",
    "for img in images:\n",
    "    image_urls.add(img.get_attribute('src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then download the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "query_directory = './img/'\n",
    "\n",
    "values = [item for item in zip(itertools.cycle([query_directory]), image_urls)]\n",
    "\n",
    "print(\"image count\", len(image_urls))\n",
    "\n",
    "for image in values:\n",
    "    persist_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter\n",
    "\n",
    "This code assumes you have a basic Twitter JSON file or directory, and will \n",
    "get all links and then images share in these tweets.  \n",
    "\n",
    "This code will skip Tweets that Twitter flags as *sensitive*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import io, gzip, os\n",
    "import urllib.request \n",
    "import random\n",
    "from random import shuffle\n",
    "import codecs\n",
    "import progressbar\n",
    "\n",
    "import os, gzip, io, json\n",
    "files = os.listdir()\n",
    "shuffle(files)\n",
    "files = os.listdir('timeline')\n",
    "files = ['timeline/' + x for x in files]\n",
    "links = []\n",
    "for file in files:\n",
    "    with io.TextIOWrapper(gzip.open(file, 'r')) as infile:\n",
    "        for line in infile:\n",
    "            if line != '\\n':\n",
    "                tweet = json.loads(line)\n",
    "                if 'possibly_sensitive' in tweet.keys():\n",
    "                    if tweet['possibly_sensitive']:\n",
    "                        continue\n",
    "                if 'media' in tweet['entities'].keys():\n",
    "                    for pic in tweet['entities']['media']:\n",
    "                        if pic['type'] == 'photo':\n",
    "                            u = pic['media_url']\n",
    "                            links.append((u,tweet['id_str']))\n",
    "print('number of links',len(links)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will download these files in parallel using all of the cores on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This will multiprocess the download (much faster)\n",
    "'''\n",
    "\n",
    "import multiprocessing\n",
    "import random\n",
    "\n",
    "def download_link(link, ID):\n",
    "    u = link\n",
    "    Type = u.split('.')\n",
    "    Type = Type[-1]\n",
    "    Name = 'img/'+ str(ID) + '_' + str(random.randint(1000,9999))  + '.' + Type\n",
    "    try:\n",
    "        urllib.request.urlretrieve(u, Name)\n",
    "    except:\n",
    "        print('error')\n",
    "\n",
    "cores =multiprocessing.cpu_count()\n",
    "pool = multiprocessing.Pool(processes = cores)\n",
    "output = pool.starmap(download_link, links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Flickr\n",
    "\n",
    "After getting an API key with Yahoo, I used the code below to scrape Flickr images:\n",
    "\n",
    "First authenticate and run query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import flickrapi\n",
    "import urllib.request\n",
    "api_key = u'XXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "api_secret = u'XXXXXXXXXXXXXXXXXX'\n",
    "\n",
    "flickr = flickrapi.FlickrAPI(api_key, api_secret)\n",
    "photos = flickr.photos.search(tags = 'funny meme', format = 'json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then walk the API results to get URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "keyword = 'meme'\n",
    "\n",
    "photos = flickr.walk(text=keyword,\n",
    "                     tag_mode='all',\n",
    "                     tags=keyword,\n",
    "                     extras='url_c',\n",
    "                     per_page=100,           # may be you can try different numbers..\n",
    "                     sort='relevance')\n",
    "\n",
    "urls = []\n",
    "for i, photo in enumerate(photos):\n",
    "    print (i)\n",
    "    \n",
    "    url = photo.get('url_c')\n",
    "    urls.append(url)\n",
    "    \n",
    "    # get 1000 urls\n",
    "    if i > 1000:\n",
    "        break\n",
    "\n",
    "print (urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally download the pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in urls:\n",
    "    if file is not None:\n",
    "        new_name = file.split('/')[-1]\n",
    "        urllib.request.urlretrieve(file, new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tumblr\n",
    "\n",
    "For Tumblr, I used an R package that I developed in the past.  If interested, reach out to see if I can share.  \n",
    "\n",
    "### Instagram\n",
    "\n",
    "For Instagram pictures, I used the Python package found [here](https://github.com/althonos/InstaLooter).  \n",
    "\n",
    "My command was:\n",
    "\n",
    "```\n",
    "instalooter hashtag meme img -n 1000\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
